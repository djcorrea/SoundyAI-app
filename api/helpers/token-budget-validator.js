/**
 * üßÆ TOKEN BUDGET VALIDATOR
 * Valida e gerencia or√ßamento de tokens para evitar overflow
 * Implementa trimming inteligente de hist√≥rico quando necess√°rio
 */

import { estimateTokens } from './analysis-prompt-filter.js';

/**
 * Limites de tokens por modelo
 */
const MODEL_LIMITS = {
  'gpt-3.5-turbo': {
    total: 4096,        // Total dispon√≠vel
    safeInput: 3000,    // M√°ximo seguro para input (deixa espa√ßo para output)
    minOutput: 500      // M√≠nimo reservado para resposta
  },
  'gpt-4': {
    total: 8192,
    safeInput: 6500,
    minOutput: 1000
  },
  'gpt-4o': {
    total: 128000,
    safeInput: 120000,
    minOutput: 2000
  }
};

/**
 * Calcula tokens estimados de um array de mensagens
 * @param {Array} messages - Array de mensagens no formato OpenAI
 * @returns {number} Tokens estimados
 */
export function calculateMessagesTokens(messages) {
  if (!Array.isArray(messages)) return 0;

  let totalTokens = 0;

  messages.forEach(msg => {
    // System/user/assistant role overhead
    totalTokens += 4; // Aproxima√ß√£o do overhead de role

    // Content
    if (typeof msg.content === 'string') {
      totalTokens += estimateTokens(msg.content);
    } else if (Array.isArray(msg.content)) {
      // Para mensagens com imagens
      msg.content.forEach(part => {
        if (part.type === 'text') {
          totalTokens += estimateTokens(part.text);
        } else if (part.type === 'image_url') {
          // Imagens custam ~85 tokens em detail: low, ~255 em detail: high
          totalTokens += part.image_url?.detail === 'high' ? 255 : 85;
        }
      });
    }
  });

  return totalTokens;
}

/**
 * Valida se o or√ßamento de tokens est√° OK
 * @param {Array} messages - Mensagens a enviar
 * @param {string} model - Modelo OpenAI
 * @param {number} maxTokensResponse - Tokens m√°ximos para resposta
 * @returns {Object} { valid: boolean, usage: object, recommendation: string }
 */
export function validateTokenBudget(messages, model = 'gpt-3.5-turbo', maxTokensResponse = 1000) {
  const limits = MODEL_LIMITS[model] || MODEL_LIMITS['gpt-3.5-turbo'];
  const inputTokens = calculateMessagesTokens(messages);
  const totalEstimated = inputTokens + maxTokensResponse;

  const result = {
    valid: totalEstimated <= limits.total,
    usage: {
      inputTokens,
      maxOutputTokens: maxTokensResponse,
      totalEstimated,
      limit: limits.total,
      safeLimit: limits.safeInput,
      margin: limits.total - totalEstimated
    },
    recommendation: ''
  };

  // Gerar recomenda√ß√£o
  if (totalEstimated > limits.total) {
    result.recommendation = 'CRITICAL: Excede limite total. Reduzir hist√≥rico ou simplificar an√°lise.';
  } else if (inputTokens > limits.safeInput) {
    result.recommendation = 'WARNING: Pr√≥ximo ao limite seguro. Considerar trimming de hist√≥rico.';
  } else if (totalEstimated > limits.safeInput) {
    result.recommendation = 'INFO: Uso alto mas seguro. Monitorar pr√≥ximas mensagens.';
  } else {
    result.recommendation = 'OK: Or√ßamento de tokens saud√°vel.';
  }

  return result;
}

/**
 * Aplica trimming inteligente no hist√≥rico de mensagens
 * Remove mensagens antigas mas preserva contexto recente
 * @param {Array} messages - Array de mensagens
 * @param {number} targetTokens - Tokens alvo
 * @param {Object} options - Op√ß√µes de trimming
 * @returns {Array} Mensagens trimmed
 */
export function trimConversationHistory(messages, targetTokens, options = {}) {
  const {
    preserveSystemPrompt = true,
    preserveLastUserMessage = true,
    minMessages = 2 // System + √∫ltima do usu√°rio
  } = options;

  if (!Array.isArray(messages) || messages.length === 0) {
    return messages;
  }

  // Clone para n√£o modificar original
  let trimmed = [...messages];
  let currentTokens = calculateMessagesTokens(trimmed);

  // Se j√° est√° OK, retornar
  if (currentTokens <= targetTokens) {
    return trimmed;
  }

  // Separar mensagens
  const systemMessages = trimmed.filter(m => m.role === 'system');
  const conversationMessages = trimmed.filter(m => m.role !== 'system');

  // Garantir que h√° mensagens de conversa
  if (conversationMessages.length === 0) {
    return trimmed;
  }

  // Identificar √∫ltima mensagem do usu√°rio
  const lastUserIndex = conversationMessages.map(m => m.role).lastIndexOf('user');
  const lastUserMessage = conversationMessages[lastUserIndex];

  // Come√ßar removendo mensagens mais antigas (exceto system e √∫ltima do user)
  let availableForRemoval = conversationMessages.filter((msg, idx) => 
    !(preserveLastUserMessage && idx === lastUserIndex)
  );

  // Remover do in√≠cio (mais antigas) at√© atingir target
  while (currentTokens > targetTokens && availableForRemoval.length > minMessages) {
    // Remover a mensagem mais antiga dispon√≠vel
    const removed = availableForRemoval.shift();
    
    // Recriar array trimmed
    trimmed = [
      ...(preserveSystemPrompt ? systemMessages : []),
      ...availableForRemoval,
      ...(preserveLastUserMessage ? [lastUserMessage] : [])
    ];

    currentTokens = calculateMessagesTokens(trimmed);
  }

  // Log do resultado
  console.log(`üìä Token trimming: ${messages.length} ‚Üí ${trimmed.length} mensagens | ${calculateMessagesTokens(messages)} ‚Üí ${currentTokens} tokens`);

  return trimmed;
}

/**
 * Prepara mensagens com controle de or√ßamento autom√°tico
 * @param {Array} messages - Mensagens originais
 * @param {string} model - Modelo OpenAI
 * @param {number} maxTokensResponse - Tokens m√°ximos para resposta
 * @returns {Object} { messages: Array, budget: Object, trimmed: boolean }
 */
export function prepareMessagesWithBudget(messages, model = 'gpt-3.5-turbo', maxTokensResponse = 1000) {
  // Validar or√ßamento inicial
  const initialBudget = validateTokenBudget(messages, model, maxTokensResponse);

  if (initialBudget.valid) {
    return {
      messages,
      budget: initialBudget,
      trimmed: false
    };
  }

  // Precisa fazer trimming
  console.warn('‚ö†Ô∏è Token budget exceeded, applying intelligent trimming...');

  const limits = MODEL_LIMITS[model] || MODEL_LIMITS['gpt-3.5-turbo'];
  const targetTokens = limits.safeInput - maxTokensResponse;

  const trimmedMessages = trimConversationHistory(messages, targetTokens, {
    preserveSystemPrompt: true,
    preserveLastUserMessage: true,
    minMessages: 2
  });

  const finalBudget = validateTokenBudget(trimmedMessages, model, maxTokensResponse);

  return {
    messages: trimmedMessages,
    budget: finalBudget,
    trimmed: true,
    removedCount: messages.length - trimmedMessages.length
  };
}

/**
 * Gera relat√≥rio de uso de tokens
 * @param {Object} budgetInfo - Informa√ß√£o de or√ßamento
 * @returns {string} Relat√≥rio formatado
 */
export function generateTokenReport(budgetInfo) {
  const { usage, recommendation } = budgetInfo;
  
  return `
üìä Token Usage Report:
  Input: ${usage.inputTokens} tokens
  Max Output: ${usage.maxOutputTokens} tokens
  Total Estimated: ${usage.totalEstimated} tokens
  Model Limit: ${usage.limit} tokens
  Safe Limit: ${usage.safeLimit} tokens
  Margin: ${usage.margin} tokens (${((usage.margin / usage.limit) * 100).toFixed(1)}%)
  
  ‚ÑπÔ∏è ${recommendation}
  `.trim();
}
